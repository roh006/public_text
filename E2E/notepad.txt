SET UP FOR SUCCESS
• Establish a rhythm. Leaders should schedule a recurring meeting. Consider bi-weekly as a starting point but 
adapt based on each colleague’s need.
• Be punctual. Start and end the meeting on time—be respectful of each other’s time.
• Make a commitment. Don’t miss or move the 1-to-1 meeting unless necessary. 
• Come prepared. Colleagues should come prepared with topics to discuss and ask for feedback. 
• Make it matter. For leaders, use 1-to-1 meetings to seek and share timely feedback, provide support and 
recognition, and check-in on your colleague’s holistic well-being

1. FOLLOW UP Follow up on actions and commitments from the previous 1-to-1. 
Is everything on track? If not, explore how you can make progress.
If something is not on track, then explore how you can make progress

TOP OF MIND
What progress are you making on your goals,  projects, career and development plan, etc. What are you learning, what are your successes?
 
Colleagues should share an update on projects they’re working on, progress against 
performance goals or their career and development plan, any key upcoming 
meetings, etc. 
• How are you feeling about projects? How are things going? 
• What are your successes and achievements?
• What have you learned? Where do you see opportunities to develop?
• What do you want to give or seek feedback on

SUPPORT :: 

What challenges or obstacles are you facing and  what support do you need?
Think about any challenges or obstacles you are facing in your projects, towards 
meeting your goals, or progressing in your development: 
• What help, direction, or answers do you need and from whom?
• Are there other experiences or projects you want to explore to support your 
development?

Action:: 
ACTIONS
What are the next steps for both of us? Who is doing what and by when

What are the next steps for both of us? Who is doing what and by when?
=============================== Havard======================================
Throughout the program, 
participants will be empowered to put what they’ve learned into practice and get 
feedback to help them continue to grow.
The program is structured around participants setting a relevant, measurable, and 
achievable goal and preparing them to successfully demonstrate or “Showcase” 
their new skill with their People Leader. During the experience, colleagues will:

Establish customized goals and a roadmap
• Access carefully curated learning activities
• Attend live, virtual moderated discussions
• Connect, practice, and give feedback with fellow peers

 Seek Feedback From Your People Leader
Feedback is all about having each other’s back and enabling each other to excel. The Feedback @ Amex Guide offers a suite of resources to help all colleagues 
build these everyday skills.
Once you’ve selected your skill pathway and challenge choice, here are key questions to ask your People Leader:
• “Do you feel that this skill pathway and challenge choice supports my Career & Development Plan?” 
• “How will my development of this skill pathway and challenge choice help me to deliver on my goals, our Framework for Winning and embody the 
Leadership Behaviors?”
• “What other development opportunities do you think I have? Do you think a different skill pathway or challenge will better help me achieve my Career & 
Development Plan objectives?”

Challenge Choice 1: Sharpen the Art of Influence
Build support for ideas and compel others to take action
This challenge will help you recognize the  barriers that make influencing people difficult  and learn strategies to overcome those hurdles. 
During this challenge you will develop a proposal  and leverage influence techniques that motivate  others to take action. For your Showcase, you  will pitch your idea or proposal to your People 
Leader and get their feedback
Example showcase topics
• Pitch an idea to promote/improve collaboration on your hybrid/remote team.
• Propose a way to improve cross-functional collaboration with another team.
• Propose a way to accelerate new colleague onboarding on your team.
• Promote a process change that would drive efficiencies.


Challenge Choice 1: Leverage the Power of Data
Solve a problem using data analytics and understand various techniques and steps required to generate insights.

In this challenge, you will solve a real-life  business problem using critical thinking and data analytics. You will understand the problem, create a hypothesis, and then 
generate insights using analytics, based on the key steps from Thomas Davenport's model of critical thinking. 

Example showcase topics
• Identify the patterns in consumer trends for the last few months to identify what could have led to lower engagement towards a service or solution.
• Describe a business challenge you are currently facing, a hypothesis on why it is a problem, gather data, conduct an analysis, and present recommendations based on the data.
• Present your analysis of the impact of a recent decision or course of action that includes a data-backed recommendation – knowing what you know now, what would you have done differently?
• Present a work-related problem or idea you have; use data to support why you think it is a problem worth solving.
• Present your key recommendations on pain points or opportunities based on geography/market unit/consumer segment data analysis

Enroll if
• You want to improve your problem solving or decision-making by analyzing available data. 
• You regularly present information to others and want to enhance the data story telling of your 
analysis and present your recommendations backed by a robust problem-solving process.
• You will have an opportunity/forum to make a presentation or a recommendation regarding a 
decision or problem you are solving using data-backed insights.
Don’t enroll if
• You do not have access to data to solve problems or make a decision.
• You do not present data-backed decisions or recommendations to stakeholders.
• You consider yourself at an expert level in using data analysis and statistical techniques. 

Challenge Choice 2: Drive Decision-Making with Data
Use data to tell a story to decision-makers and stakeholders so that they will take action

In this challenge you will work on an upcoming presentation you need to deliver using powerful techniques of visualization 
to tell your data story. In our attentionchallenged world, communicating analytical results in a clear, attentiongetting way is critical. You will learn how to communicate data and insights in a way 
that increases your ability to persuade and inspire your stakeholders to confidently make decisions based on your recommendations.

Example showcase topics
• Identify the patterns in consumer trends for the last few months to identify what could have led to lower engagement towards a service or solution and present the same through telling data story and visualizations.
• Support the description of a current business challenge through data story and visualizations.
• Create a compelling case for action by communicating your analysis through compelling graphs.
• Present a work-related problem through visualizations and narrate a powerful data story.
• Present your key recommendations on key pain points or opportunities using strong data visuals in a Quarterly Business Review.
Enroll if
• You often make presentations with a lot of data and visualizations. 
• Currently you do not use or are not content with the visualizations in your presentations. 
• You have received feedback in the past that your presentations can be very data heavy. 
• You would like to increase your influence in presentations by leveraging data-based storytelling and powerful presentations.
Don’t enroll if
• You do not see yourself working with data and visualizations in your current or future roles. 
• You do not need to influence stakeholders using data. 
• You consider yourself at an expert level in presenting data visually.


Challenge Choice 2: Gain the Respect and Interest of Your Audience
Deliver presentations that elicit attention, respect, and the action of others.

During this challenge your will develop a  presentation and practice delivery techniques  that will enhance your ability to present with 
confidence and engage an audience. At your  Showcase, you will deliver the presentation to  your People Leader and get their feedback.
Example showcase topics
• Present issues related to returning to the office that ends in a recommendation to 
mitigate concerns.
• Present a recent decision that was made, including the problem that necessitated  a decision, the alternatives considered, and the rationale for the final decision.
• Present your analysis of the impact of a recent decision, including a recommendation – knowing what you know now, what would you have done differently.
• Present a work-related problem or idea you have – why you think this problem is worth solving or the idea is worth implementing.
• Present the results of an After-Action Review or a Quarterly Business Review.

Challenge Choice 1: Drive Your Path
Develop your network to expand your opportunities.

During this Showcase you will demonstrate the  actions you have taken to build your network. You will share the following with your People Leader:
• A comprehensive and diverse network map you  have created that is aligned to your career goals
• One to two actions you have taken during the experience to expand your network
• Two additional actions you plan to take to further broaden and strengthen your network.

Example showcase topics
• How you have and how you plan to expand and strengthen your internal  network. 
• How you have and how you plan to expand and strengthen your external network.
• The people in your internal and external network that you could connect with that would help you achieve your career goals.
• A clear plan for making those connections and developing the relationships overtime.


Challenge Choice 1: Engage Others
During this learning experience, you  will develop a story that is intended to  engage others, pique their interest, and keep their attention by using a combination of authenticity, empathy, 
and logic in the story’s content and structure, and in the way you communicate it. Your Showcase will require that you tell your story to your People Leader and get their feedback.

Factors to consider when picking your showcase topic
• The story you tell should be one that you are passionate about, have experienced firsthand, or have in-depth knowledge of so that you can answer any questions that arise.
• The story should be work related and of interest to your audience.
Example showcase topics
• A time you gained buy in for an idea or project.
• A time your team encountered a challenge and you worked together to think  innovatively about how to solve it.
• How you made a past career decision that is important to your present role.
• A value learning experience or lesson and how that informs your work today. 

Challenge Choice 2: Connect with Others
Communicate in ways that enhance common understanding including conversations related to differences in opinion that prevented an optimal outcome, personality and communication style 
differences, recent decisions where there wasn’t alignment among everyone involved, or breakdowns in communication where emotions ran high.
Example showcase topics for a trust building conversation
• Addressing a difference of opinion with a colleague to achieve an optimal outcome.
• Addressing personality and communication style differences to enhance a working 
relationship.
• Speaking up about a recent decision where there isn’t alignment among everyone 
involved.
• Resolving a previous breakdown in communication where emotions ran high.


========================================================================================================================
Yeah. Perfect, I agree. I think you got the introduction. So today, I'm going to talk about how we're using analytics within the realm of sustainability and include some of the really interesting Climate Tech things that we're doing. I put them in different columns because there is a huge overlap, but there's still a lot of difference between what sustainability is and what climate tech is. So today, I'm going to talk about sustainability as a very holistic approach. There you have different pillars of social and governance. Also included, just to give an idea of what those pillars look like. OK, great. So the environment, social governance, or otherwise we know as ESG. These are the 3 pillars that are currently looked at to understand if an organization and the environment of the organization and its activities are deemed sustainable or not. UN released these sustainability development goals for the world, not just organizations, to aim for on how we want to improve and move towards a more positive plan and outcome. It's quite interesting to see how they can then sort of situate themselves within these 3 pillars. Not just that. it's also good to see that there is a huge overlap because they are not defined by the definition. They cannot be considered as independent, and this helps us. A good example of that is the deforestation policy, so HCC is moving towards a zero deforestation policy. That is not only concerning the environment, but it's also governance. And it's also social based on the impact that is going to create for us and the planet. But the world of ESG is extremely complicated. The landscape is really mixed with a lot of different vendors who are coming in with their own methodologies and their own assumptions of what metrics should be considered when measuring each of these pillars, which means that as a financial institution, you're bombarded with so many different types of ratings, which Are they being added up? Are they being averaged out? That definition itself is different across different vendors, and this creates a huge problem because as an investor or an analyst, you just want to know if this organization is sustainable or not. So how do you get from 1000 different confusing data points to a single understanding? Similarly, there are a lot of frameworks with which you can do reporting, and even these frameworks are constantly changing and we are moving to the new CSRD framework, which is aimed to help sort of bring some of this together and reduce this complexity. But as of now, there is still a huge amount of complexity in this landscape. You have to also think about the amount of costs that the organization incurs when they have to move from one framework to the other. It doesn't impact a large organization like HSBC to that extent, although it becomes a huge admin task, but it does impact the small organizations where affordability becomes a big question. Some of the biggest problems that we see is the limited coverage of private companies, so we do not have a lot of disclosure from private organizations because it's not considered mandatory at the moment. and also geographical gaps play a big role in this. So we do have a lot of systematic geographical gaps and there is the insufficient sector granularity. So I'll be calling the sectors the same globally. Are there different new ESD sectors within our organization? We have multiple definitions of the same sector or subsectors, so bringing all of that together and creating the right standardization on data is a big challenge. Lastly, we also don't have a standardized entity coverage and what that means is from the methodology. Different vendors might report the same entity with different ratings, or some entities might represent in 1 vendor and the other entities might present in a different window. And Lastly, we have a lot of information below, so as I mentioned, we have too many data points, sometimes meaning the same thing or sometimes going into too much detail. So what would you do as an organization and some of the things that a lot of financial institutions are trying to tackle is to design their own sustainability scoring framework. And why do we want to do this? Every bank and the relationship that it has with its clients is unique. You want to determine that that relationship is what you are measuring from a sustainability perspective, and this makes it really personalized. It also enables the bank to define how they want to bring together all of these different methodologies, have a more standardized framework that is defined, have a taxonomy that is defined and unified across the bank. This helps at least maintain some sort of governance and control within the organization itself. So how would you go about designing that? Well, let's say that you have all of these vendors. Do a lot of testing on the vendors. Determine which 1 is the right vendor for you or a collection of the global entity coverage that we have. Given that huge selection of vendors now, you want to then put all of that into a raw data store and this needs to have its own sort of injection and testing pipelines to ensure that data is regularly tested and tested for quality purposes.. You Then want to probably define your own standardized taxo You might have 1 vendor who says THG Scope 1, another vendor might say THG Emission Scope 1 and so on. So it's important to create a mapping process that sort of brings all of these together and has some sort of index thing behind it to know which value came from which vendor. Then all of this preprocessed data gets stored into a data store and then you would probably want to do a gap analysis report to really understand. Now that I've got all of these vendors together, I've done all of my preprocessing, I've tried to maintain So unfortunately, you still have a huge amount of limited coverage because as the reporting doesn't happen, that data doesn't exist. We also have the insufficient sector granularity problem because again, global definitions are not that easy to bring together and your geographical gaps would also exist. So, it's like, okay, let's put in some inference models because as the data doesn't exist, we have to use things like machine learning or even statistical models to help create some sort of data driven inference, which would mean that if we take, if we have But what that creates is a lot of bias in the organizations that are represented. Usually these are the large organizations who are public and would report, or you have geographies that are more well equipped to report on their emissions as well as the sector. So, did that create that sort of bias in the models and you need to have a more holistic view where your modeling and scoring doesn't just depend on ESG data. There's a lot more that helps us define what is sustainability and resilience. ESG Data should only be considered as part of a larger system. So, the different types of models you can think of and we'll go into details for some of them. So, we look at financial health. Resilience is, financial health is really necessary for resilience, as we've observed in the pandemic.. Those organizations who have liquidity or many in the banks can make sure that with the incoming changes, they are more resilient towards risk and they can actually fund their own transition. Similarly, you want to think about biodiversity impact. So, what is our impact on the surrounding environment? Or, also, if your operational activities highly depend on other countries, what is the dependency from a biodiversity perspective? Because the impact of that will in the end, impact the operations. Country Models, so what is the country looking like? How much risk is that country at from climate change? And also, functions and things like that make a big difference in a company's overall performance. Lastly, we also want to look at qualitative models which means things like articles, news reports, press releases, sentiments about this organization. That also helps provide a lot of the ESC data. Quantitative is just what we talked about earlier from the inference models perspective. More upcoming field is remote sensing. The field itself is not upcoming, it is pretty old and has been used for multiple different things around climate change. However, it's been picked up by financial institutions now to help enable us to have that remote view for geographies and organizations that we can't treat. Yet again, you cannot depend only on remote sensing and you cannot depend on any of these boxes on their own. It is a holistic combination of all of them.. And the weighting that we apply to each of them is a business model that you design according to the business strategy. Lastly, we found that we can also implement some sort of smart search capability across the ESC data store, which then helps us get over the information overload. So if I just want to look at emissions, instead of browsing through all of those data points, can I just use some sort of word embeddings or NLP models to help us find the relevant data points? Now with all of this data, you can create a weighted sustainability scoring model.. And This helps you then produce dashboards where you can A, show all of the data points of interest.. B, you can implement some of the remote sensing tags, to show rising temperatures, wildfires, droughts, and all sorts of risks. And Lastly, we can also zoom in to our client's asset locations and then get more information about the area and the surrounding area. So some of the things that are really useful to dive into here, the Geospatial ESD and Biodiversity. So biodiversity wasn't looked at that much until quite recently. WWF released this really useful report, which actually helps you compare the direct and indirect measurement. Indirect Measurement this year is what companies report. Direct measurement is what we measure with remote sensing. So just for, from a remote sensing, deep dive perspective, remote sensing is when you observe an area of interest remotely. So that could be things like satellite images, or it could be drone images, or it could also be emissions data that is captured through towers and flights, UAV data, and so on.. All of those data points are then looked together through a machine learning model. And then this helps, and we also use a lot of historical measurement data. So things like carbon dioxide, we have a huge data set of historical emissions. And that helps us understand what the trend is looking like. You can also do things like measuring the vegetation, looking at the company from an asset level perspective, and then going around and looking at what is its impact on the environment around. The Biodiversity part then looks into the species diversity. So what different types of species exist? Are any of them endangered? Do any of them exist in a protected area? that should not really be used for any sort of company activities? So all of that data is measured, and then given an allocated rate, depending again on the company's policies, as well as the scoring methodology.. And that helps us then aggregate towards a more consistent, sustainable score.. This does suffer from things like asset location data. We do not always know where our clients are operating. And that part, especially things like land ownership, is a very confusing and complicated data space to work with. Similarly, in the temporal inconsistency, you think that satellites follow the same path, but that's not always the case. And you have a lot of geometrical complications because they're moving a lot. And the area of interest has a lot of different shapes, and you need to use a lot of geometrical mathematics to bring it all together and average out the shape. So some of the really fun things that we are working on is the measuring impact. So we look at climate models. We look at how the land cover changes because of human activities. So that could be how much deforestation has occurred from the Amazon rainforest, as well as things like we want to understand what the impact of a natural disaster is on the areas that our clients operate in. We also want to look at things like understanding what is the supply chain demand assessment. If you are very focused on more in the agricultural financial institutions, there is a lot. They even look at things like profitability for the land that they're financing, as well as measuring based on the profitability and crop rotation recommendations, what should be the seeds that they order for the next few years. A lot of the farmers in the UK already do this with remote site thinking capabilities. And Lastly, designing policies. So how do we design a more data driven policy that whose implementation and execution can be monitored by machine learning as well as data collection? So we are actively working on implementing our zero deforestation policy, and we want to be able to use our remote sensing capabilities to help us make sure that we can monitor that over time. Some of the other things that we are doing is also measuring real time emissions. So, and what I mean by that is geographical emissions, which is not the same as what a country, so a company reports. A Company reports emissions based on the sum of their emissions for all of the subsidiaries. And perhaps sometimes they even could use hedging to show that it's actually being produced. Whereas your geographical emissions are actually measured from the satellite as it's served in the atmosphere. what is the impact right now that we can observe? And we use deep learning models and we use things like images which show what does this look like during daytime? What does it look like at night? So is it operating a lot during the night? What is the impact on the vegetation health over time? What is the increase, decrease in temperatures? So we can measure all of these data points over time and it gives us an understanding of trend and how it impacts the carbon dioxide estimations.. We do this in order for us to then help provide information about things like what is the impact of a coal mine in an area around it? And then does this reduce when we look at the solar farms or wind farms and as they grow in number, what is the overall impact over time? We also, look at it because a lot of these temperature data points are connected to natural disasters. Things like flooding, wildfires, droughts, they're quite connected to the temperature increase and the concentration of things like carbon dioxide, nitrous oxide in the atmosphere. So if we have better predictions, we can input them into different climate models. and we can also look at things like historical flood data, historical wildfire data, and produce this sort of interactive map. What we wanna do is for our front line or any of our plan analysts to be able to interact with this map to zoom in to where they believe the clients are located. to get more information around. Okay, this is the flood probability here. This is the percent number of floods that have occurred historically and the sort of depth. The colors in the map here indicate the intensity of the flood and how probable it is. So the darker the color, the higher the probability of flood. We are also looking at monitoring biodiversity over time and we started with deforestation. This is also the model that will help us work with the deforestation policy, but we are thinking about also quantifying this impact and putting a dollar value to it. So if we measure what is the carbon that is sequestered or stored by a forest, and if there has been some sort of deforestation over time, how do we quantify that to what we have lost from a sequestration and storage capability? And this helps us design a model that will almost have a better accountability scenario around it. So we're starting with the Brazilian Amazon and building our models over that. Last thing. We also look at the Sustainability Report. So a lot of our sustainability reports at the moment are quite manual, where analysts have to go and read all of these massive documents. And we've been working on how do we automate this process so that it helps reduce the time that analysts have to do this. So what that means is you can go through each of these sustainability based documents and detect the tables that report the metrics of interest, automatically get the data out and then answer the questions that are based on these reports. Each of these reports is unique and they're all different designs. Sometimes the tables are HTML, sometimes they're just images. So it's a very complicated collection of NLP models that we need to work on. This is one of the ongoing active problems that we are trying to solve. We've been looking at different types of models and seeing different models and seeing which ones can answer different types of questions and maybe they go wrong. And there's a lot of trial and error methods. And in order to fine tune all of this, we are facing a problem of labeling all of these documents.. And as NLP problems generally face, you have to basically go and manually mark each of these tables so the model knows that this is the right answer. This is a very time consuming process at the moment for us. But when we do get a clean table, and this is synthetic data, it's not real, but we also wanted to see what is the model performance. We use Tapas here, which is an open source model. Similarly, the models before DevNet and LayOut are also open source models. So if we were to have 1 report measuring emissions across different countries, would our model be able to answer the questions to sum them up? So in this case, we want to see what is scope and emissions of the model. We Want to see what is scope and emissions in the United Kingdom and India. It's able to get that and it's predicted. The logic that the logic that should be used is summations. Similarly, we asked that for UK and IN, but with the ISA codes, and the model hasn't seen enough examples to understand that this is the same as the United Kingdom, so it doesn't get the ISA codes. So these are the product problems that we have to face and try to solve them. And we try to break it down into really small questions to iteratively improve our models. So that's pretty much all of what I wanted to share and I'm happy to take some questions.


Different locations in the machinery, all the processes that they're trying to run, or setting up cameras to go and see how processes are running, and then using AI to understand what's happening with the process or the factory, and infusing that intelligence back into decision making. So that effectively is what we call a smart factory in this space, right? Now, the 2nd piece which I talked about is pretty traditional in this space, which is the supply chain part. And how do you use analytics and AI to optimize supply chains, look at procurement, look at logistics, optimization, and parameterization? The 3rd 1 is on retailing, which obviously I think everyone on this call will understand from a retail standpoint at least. Are you talking about customer analytics, understanding purchasing patterns, cross selling, upselling, understanding share of wallet with the customer? It may be B2B, or it may be B2C as well, but the idea being this is close to what high transaction data we see in banking. The 4th Area is what we call core functions of an organization, So how do you use analytics and AI in finance, how do I use it in Talent for Human Resources, how do I use it for defining a corporate strategy, right? So 4 key pillars for analytics and AI, which we have seen in the manufacturing and AI industry, right? And the underlying base for it is basically setting up a unified data platform, which is built on the cloud for the clients to do it really quickly and quickly, so we don't worry about any legacy systems and how to manage and build AI models on those. we basically skip a step in between, not setting up a data warehouse, we directly go onto the cloud, set up data marts, do the data integrations, and start building our AI models on top. So it's one of the benefits of actually going through that phase of rapid scale up of AI analytics in this industry. So the way we do it, like we typically do in IBM, obviously there are different layers to how the integration happens from, if you go from the bottom, different source systems that they have, which is IoT or Internet of Things, basically sensors which are put in which create a constant stream So things like setting up a temperature sensor on a machine or a vibration sensor on a machine, and feeding it into a data platform through our own layers of ingestion. data security, creating a curated data layer, and then we bring on top of it our own IBM accelerators to fast track the AI development on top of it. Some of these are really unique accelerators in the sense that they are designed for process modeling. Typically in analytics and AI use cases. in banking, we have discrete applications of AI, so you have classification problems or you predict a continuous variable, but in this case you need to do process simulations. You need to simulate what a process looks like. And a completely different set of AI tool kit is required for that. So a lot of accelerators that we have built in are brought in as part of this platform, and then you have a standard AI engine on top and plug and deploy analytics use cases. So this foundation layer is something that was built for a lot of clients, right? Now having said that, I have talked about a wide variety of areas, or 4 areas that I talked about. What does it have to do with the call that we are on today, right? So I put some thoughts in on 3 or 4 areas where I think it can be used for banking, but I love to hear from this group, either through chat or through questions towards the end of the session. Where else do you think some of these capabilities could be deployed, right? So I'll talk about some client stories now where we have done this, but 4 areas basically where I thought it would help. 1. To help you manage your operations more effectively, whether it is in the branches, in the back office that you have, or in the mid office, or even in the call center. And I'll elaborate on this. The idea is for you guys to think through some of these lines, and then we'll discuss it towards the end. The 2nd: 1. How do you optimize locations for your ATM branch? Plenty of use cases done, but what can we do differently? The 3rd is, how do we simplify the consumption of data for executors and decision makers across the organization? So with this huge amount of data now being available in the sector,, they also started to understand how executives absorb and assimilate information to make decisions, right? How do you do it in a simplified fashion so their life doesn't become very complicated? The last 1 is around the improving security, right? So I'll not touch upon what that is, but I'll let you guys think through as we talk about these client stories. Yeah. So moving on. So 3 client stories I wanna talk to you about today. The 1st is a steel manufacturer I was talking about where we created an entire digital twin of a blast furnace. The 2nd is around a computer vision use case that we did to identify anomalies and automate the entire process. And 3rd 1 is for a cement manufacturer where we used energy to actually automate some of the things that SAP does as part of their ERB deployment. So those are the 3 areas I wanted to talk about. So let's maybe, Konso, let's begin with the 1st 1 on digital Twin, if you can, move on. Right. So basically, when we think about creating a digital twin, it goes through 5 different stages and eventually ends up being called a Cognitive Smart Factory. But the idea is,. so I'll give a quick 1 minute overview of what a blast furnace is. It's one of the most complex chemical processes that are there across the world. And because nobody actually knows how steel is made inside a blast furnace. We're talking about temperatures close to 1500 degrees Celsius. So there is no direct measurement of what's happening in that blast furnace that can be done. But to manufacture steel at the end of the day, you get a slab which comes out which you start using. At the top, you start putting in your raw material. And you need to control that process using some of the indirect sensors you put in, which is basically things like pressure inside the furnace or temperature on the outside walls of the blast furnace that you can observe. Now, can I use all of that huge volume of streaming data to create a digital replica of that blast furnace and then optimize the operations remotely? So the way it typically happens in this journey that we've gone through with the client is, you started looking at an exploration of where IoT data can be captured, setting up and probing the instrumentation around it and putting these instruments across the blast furnace, connecting it to a source system where Which is, I mean, just to give you an idea of scale,, this is somewhere close to a 20 floor high building the size of a blast furnace that is typically there, some of the large ones. Are you looking at temperatures like I talked about, 1500 degrees? Celsius, and you're talking about close to 10 metric tons of steel being manufactured on a weekly basis, right? So, Kunthal, if you go to the next page. So this is an example of a use case where we created a digital twin. This is not exactly the 1 which we did specifically in this geography, but the idea was basically, we used the source data that was there from the blast furnace and created, 1st of all, AI models which predicted what was happening inside the blast furnace. So we don't have a direct measure of it, but you can predict, say, for example, how a particular process is flowing, right? Or how operations are working inside a particular unit. You can completely model it using the simulation tools and AI modeling that we did. Once you've done this and you've created a digital replica of a process, you can then run an optimization on top of it to figure out how it can be done in an optimal manner. So you can optimize and reduce costs. You can optimize the throughput, as well as minimizing the turnaround time for any downtime that is being observed by the blast furnace. So in essence, 1st you create a digital process replica, then you put AI models on top of it, and then finally run an optimization which runs over and above that, right? The challenge for this client was, they were facing a huge amount of operational inefficiencies because of the inputs that were coming in, right? So there were variants in raw material that were there. there were variants in the process that were running, which led to wastage of constant time for the resources. And for us to deploy this on the environment, we basically set up a cloud instance of the platform, linked all the data together, and built these simulation models on top of it, right?? So that's the use case that we did. Now, I'll just pause for 10 seconds for you to think about how we apply it to banking and to operations. So any place where you have processes which are running, which can be directly or indirectly measured, and where you wanna model it, and optimize it for cost savings or for throughput, that's where the same capabilities can be applied, right? So I'll not talk about which examples, but let's discuss it in the chat or postfactor questions that we have. So that's the 1st case study that is there. The 2nd 1 is about computer vision, right? And I think this is something which, I think, intuitively looks pretty straightforward, and I think it's been done multiple times. But I Think What? I wanna highlight the nuances typically when you're trying to solve a computer vision use case, then what kind of areas it can be applied in, what complexities you have to deal with because of the problem that you're trying to solve. So this is something that we did for an automotive manufacturer, where we were trying to figure out on the shop floor, if there were anomalies that were there, or the quality of output that was coming in, rather than a person or a set of people standing and observing and monitoring what product was can we use AI to identify anomalies through computer vision? So the idea is to deploy these AI models on the edge. So we have a camera which is situated in the shop floor, the AI model runs remotely on it. Based on the images that are being captured, it can flag up, highlight and generate an alert whenever there is a drop in quality. This is enabled primarily through a combination of different technologies. I think with 5G coming in, there is a huge scope for actually increasing use of these technologies to be able to run high throughput video cameras in remote locations, where you can't have people monitoring or checking for things, and basically using AI to automate that entire process. So we call it Visual Inspector as an app, which is made available, and we run it through these edge devices, and you can tackle different kinds of problems with that. So just to give you an idea, Kuntal, if you go to the next page, I'll touch upon the typical use cases that we see. So inherently in any video feed that you get, the way we analyze it, is analyze frame by frame, within a frame, identifying and detecting objects. So there is an object detection process which happens, which breaks down the frames into different components, which you can see on the right hand side, where an engine block has been broken down into different components.. 2nd is you also then qualify and grade every component as to how good in terms of quality certification it So whether it is close to the threshold of passing a quality check, or you see a deformity or an improper installation, it can be identified. If there is an electrical connection, which is not put in place. So 1 Once you detect the object, the 2nd is to figure out whether it is in the right place or not, or if there is an abnormal situation, which you see here. And then you can use that to basically flag off alerts to the people who are monitoring and observing that particular assembly line or that station. And you can go very granular, or you can also use it to look at broader objects. We've done these same use cases in warehouses where you start different components, you can figure out the utilization within a warehouse, how much space there is, how the workers are, if the workers are alert or not, you can actually read expressions as well, and see and build use cases So before an accident happens on the shop floor, can you identify those and proactively push out alerts to the supervisors who can go and manage the situation? So physical objects, people, reactions and sentiments and all that can be analyzed. What we don't use computer vision for, and that's an IVM policy, is to use it for identification of individuals or for facial detection or recognition. Those are use cases IVM is a policy has stopped doing around 4 years back. But anything outside of that, which is to do with object detection or with identifying safety related issues for individuals, those are still something that we partake in. So I'll take a pause, I mean, after this use case, try and think of scenarios. where in banking, where you can use computer vision to either identify anomalies or to even generate data for analytics, right? So you might have video cameras in different places. Can you use the same video feeds to generate data for analytics and AI use cases further down the line, right? And that's all the hint I'll probably give on this before we move on to the next one. So that's the 2nd use case, which I wanted to take you back to. Well, we have a couple of questions on the case you have just talked about. So I think 1 question is what is the percentage of AI in the prediction that you have talked about? What are the methods that are used to optimize the process here? These are the 2 questions that have come, right? So what's the percentage accuracy? I mean, that was the question. Yeah, sure. So for example, when we're doing process modeling, when we're trying to predict, so for example, in the blast furnace, we don't necessarily measure accuracy because you need to define a threshold for it. RMS For a process where the output is between, say, 1 to 5, the RMS is a low 0.3 or 0.4, right? So you can look at an accuracy of anywhere above 85%, which is pretty decent for simulation modeling. For Optimization, you can use a variety of optimization techniques like MILP or NLP, nonlinear programming. But in this case, because the process for a blast furnace is so complicated, we actually are using a combination of deep learning and optimization techniques to model it, right? So we've used recurrent neural networks to model the process. And then we've also used optimization MLP on top of it. To bring all these AI models together, so think of optimization as an umbrella, which pulls in the outputs from, in this case, there are more than 30 of the AI models that have been built. So from 30 or the AI models, take the simulation of what's happening in the process, put an umbrella of optimization on top, and then run different hundreds of simulations to figure out what's the optimal input for an operator to use to maximize yield or minimize the cost, right? So that's an optimization, doesn't necessarily have an accuracy metric. What we do measure is in how many instances did the operator actually use the recommendation or did they 
not use the recommendation. And we then have a...

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
We train the conceptual models on their specific catalog, whatever client we are working with. Once we have the models trained, we can create the features out of those images. So in apparel, it's not just about the color and the price, right? or the size. It's a lot about the style, the fabric, the patterns, which is there on there, or the length of the arm and the design of the suit and all of that.. And it has to be specific to the trend, which is kind of. it's a really special pair, versus an Indian pair, versus anything else. So you have to have an image. So what the Convertivation algorithm does is, it looks at the image, creates a bunch of these features,, which then makes the models more intelligent in their ability to predict whether the design is going to be successful or not. And we have, this is a live use case. We have the vendor then using it and the supply chain team using it to be able to figure out when they are designing something, what would be the sell through rate of that item tomorrow. And this is so that they can then optimize the supply chain. And that leads to a huge saving in terms of the clothing that is out there, and then it's related to it and all of that. So it started with the problem of reducing waste. We came up with a solution which includes Convertivation and AI, and they're working with their supply chain people, they put it in their hands, so that they can start leveraging it. And then eventually it leads to kind of savings of that kind and hence reducing their overall footprint and kind of energy consumption as well. So I think that's another example where you can use AI and the benefits of it to do effects of collection. Is That Helpful? Any other questions? Yes. It helps. Thank you. I Think no further questions as of now, So over to you, Brinda. Thank you, that was an extremely interesting session. Love to hear about the concept of the general cost. And I'm very interested to understand how that applies across industries, especially once we hear in such detail about the work of how to make these kind of ideas reality. So Taking away from the last session where last few minutes were spent on sustainability and keeping in mind HSBC's ideas of trying to achieve net 0, our next session is actually centered around the theme of sustainability. And in this day and age where we're extremely climate conscious, achieving sustainability is the need of the hour. I mean, we have to know what kind of business impact we're making and......to the topic. And I'm gonna invite Roshni Jowri, who's a climate tech expert, though she has experienced a lot of AI across multiple sectors. She's passionate about sustainable and ethical AI solutions, and she's gonna today walk us through what the world is moving towards in the climate and sustainability here.. Over to you, Roshni. Thank you for joining us.

============================  Everly discussion==============================
I just got back a few hours ago so I am still trying to go I am so sorry, I am so sorry That's fine, I was going to work today but I only got back at 8.45 and then work started at night So it's 12 o clock now I know, I Apart from that, Happy Diwali to you and your family I Hope you have a good time with your family as well. Vrashini, couple of questions I have related to the work and apart from that I don't have anything so probably I will not take too much time Just a 2nd I just pull out my notes No, just kind of a note, I just want to understand like a Vrashin Yeah, it's better Okay, So the role profile itself is going to be the machine learning lead for my India team that's going to be for ESG Analytics. Mostly we will try to make sure that everybody is in Bangalore but there is potential that some of your team might be in Hyderabad depending on Sometimes it could be that we are also from policy as well. So 1 of the projects we did now is on deforestation. so how to help HSBC build a better deforestation policy by providing them with data Sometimes it's just a simple analytics, it doesn't have to be complicated machine learning. But even that can be a game changer and a legacy back because they will use really stupid things that we just make them make better decisions. So from that perspective, our aim is to enable the various teams at HSBC to be able to use our ESG data to provide analytics capabilities to help them get that. Then your business goal and just from your focus of what you need to achieve then there's multiple parts to it. So the 1st part is definitely obviously the projects that are to be the success of those projects. But also to build components that are reusable. So 1 of the things that we really want to do is what I was telling you before is to build our standardization process and how we want to deploy machine learning. So what is that end to end from front door like getting a demand coming in or we have our own research project that we want to focus on What Does that look like from somebody starting on experimentation on maybe an external notebook, then bringing that into the HSBC network? Then Getting back from a notebook to an actual proper IDE project and we actually just improve this process and have a template? Which is you know, like just like a starter template, you just download this from the repository and then build your code over that so that we enforce standardization. And don't let people come up with their own way. The more simpler, we make it, the less complicated people do things and add their own stuff to it. So how do we standardize deployment of machine learning and I think creating that sort of reusable templates and pipelines can speed up our velocity quite a lot. So that is really improving the velocity of deployment. How do we improve operational efficiency within the team and then the other things we really want to do is we build So Many people build so many things across the bank and so bit in Spain And even though there are people who are trying to bring things together Like Today I had another meeting put into my diary because someone wants to And again, it's like yes, but if you put it in your repository it's still not available to me. What we are doing is to create a centralized library so that I can install it and that should be the focus. It should not be code lying in a different repository, here means it could be in a completely different cluster that I won't have access to, so I have to spend time How to do that is completely a mystery in HITCC and it's stupid. So we need to sort of have this review process and create a way where we can actually have these centralized repositories where people can just download libraries or add to the libraries. So how do we create that for some of the remote sensing stuff that we are doing? There are sampling techniques that the team has written on their own and there are various methods on how they are satisfying the labeling. If we can improve that and have that available as libraries without those effects. So there is a lot of operational efficiency that I want the team to focus on because you need to have that capacity to take a step back and say okay, this is where we are taking too much time. So let's improve this process here. And then initially it's going to be a lot more. Let's pick this project up, let's use it as our guinea pig and then let's make sure that everything we build around it is something that can be used. And making sure that it's also in line with as much as we can get away with industry standards. And I don't mean finance industry standards, AI industry standards because finance is a great story. We have to give a thumb out of that era. But yeah, that's kind of the goal. In short, it is to enable analytics capabilities in DSG, a wider goal. More Focused goal for us is to improve the operational efficiency in our team for machine learning deployment. And then the overall success of the project within analytics and in a way also mentor the internal India team. Yeah, sure. Thank you. Thank you. Just Giving me all you will cover all the doubts. I have created a couple of questions. But within the questions, you have covered all my answers with your answers, I have covered all my questions. So, what do you think? So, you have already talked about the challenges. so I don't come up with a similar kind of question. What do you think about the challenges I faced in this part? I Have a 1 doubt because there's a 2 high inflation because I'm also leading a team from Indian American Express. I know there's an onshore, there's too much impact and we are cutting off the vendor partners, and we are outsourced over to India. So, is there any impact on the inflation side, which is a cause for concern in terms of the job role point of view? There is definitely a huge push in just with your overall shift focus to Asia. Now I don't know if that is primarily because of all of the stuff that's happening, I think in China. I think the focus is more on India or the split the 2 parts. whatever. But the overall general business focus is for us to move to Asia. So that's why you have a huge team relocation going on. we have loads of our likes to have to move to Hong Kong. And then even my own team got this funding so my team doesn't really exist anymore, because they sort of got this all. And then towards Hong Kong as well. And from our East Analytic and Overall group. Yes, the biggest focus is actually building to work in India, but if they have Bangalore and Hyderabad. I feel like the people who are going to who might possibly be in trouble are the ones who are in the UK. They're in their money. UK UK Team they're the ones who are in, but that definitely means that we have to cut off our spending towards consultants in the UK. So that's a definite no go. So we have been given that you have to reduce any more spending during that time. And then there's a lot of this on how do we save money on vendors as well. So that's again something we can use to our benefit. Why, because a lot of our projects are looking at open source data. Yeah, we can build things from it and that's something that we can say okay with our project and save you money from vendors. That kind of thing. Another question. Another part of the question is, so you should be my boss, and you should be my leader, right? Because why I'm just saying that because when I had a word with the wash, she said that someone from India, you have to report. So I was totally clueless. So that's a, I just want to clarify, because the way you have experience and exposure in the ML and AI and I always look forward to learning from you. So that's the 1 thing which I just probably think, okay, if you're not my boss, then I have to think about it. So that's the 1 thing. So your entity manager, I think that's what they call will have to be in India, which means all of the admin stuff and those sort of holidays and those sort of things. So, yeah, that's the law, but as far as when it comes to projects and how we work together in a team and what projects you'll be working on and ways of working and all of that stuff will come from our team through me. So any of the HR admin, all of those things that will have to be from India because the other rules are different from that. Okay, 1 more thing on the last is probably my last question. So on the ESG side. So what are the? So, I know because ESG is totally a different pillar. I think about the banking side. So, what is the scope, what is the future plan, and how much sustainability is on the ESG side? It's just busy. Think about it in this particular pillar. Definitely. In other words, HSBC has promised to move 750,000,000 to 1,000,000,000,000 USD in funding towards sustainable projects. That's their net 0 goal. And so, everything we do sort of contributes to it in one way or the other. So from a sustainability focused project, there's always going to be something going on. I want to try to make sure that the projects we pick up are something that have a lot more AI focused on, and they have more innovation because honestly, otherwise I'm bored. So, I don't really want to do some boring stuff. But again, we can get the vendors and consultants to come and do the boring stuff, but we should do the interesting stuff. So, we have to try to see. But then we also need funding. So it's that game that you have to play. If someone offers a lot more money that might then pay for another person to join the team or for us to get a visualization expert. Then maybe we have to do it. But we discussed that as a team. I don't know those decisions. But the dictators were very democratic in the team. We all do it. We all have quarterly sessions. We Just finished 1 Now where we decide, okay, how are we going to pick projects? How are we going to design here? How are we going to design things? And then we say, okay, we're going to experiment with this this quarter. If it works, it's great. If it doesn't work, we'll review it and change it. So everything in that sense is quite independent. Obviously, if something comes from the top where we just have to do this project, then sometimes we have to do it. But yeah, otherwise, we're pretty good. So just to give a more focused answer, definitely. We have a lot of projects, but in Egypt sustainability and climate change are considered different things. I don't agree, but that is debated. And so you will have a lot of climate risk analytics and climate change and you have climate change and all that sort of stuff as different things. And then you have us just more on the ESG focus, but ESG uses data points at the end of the day. We will use machine learning models to help us cover the gaps in the data, to build insights over the data, and to provide those insights to the climate risk analytics team. The Physical Risk team can get more information from us about geographical features. So if you have flooding happening, the vendors right now, they just give us flood probability and then they say 100 years, this is how much water will increase and we pay them 2 1000000 for that, which is insane. Yeah, So we want to build our models and we also provide flood probabilities, but we give them elevations, flow, we give them historical, we give them basin, the soil quality, the vegetation around.. And we want to eventually see if we can replace the vendor or reduce our dependency on So how can we improve the data? So we use remote sensing and those sort of things to be able to get that. So is that the experiment? How can we either replace the vendor or enrich the data that the vendor gives us? So then, you know, you have to build trust and things like that with the stakeholders because doing something like this, like wanting to do something in house is very new for the bank. They don't generally like to do it. They just want someone else to do it and then they just want to buy it. So you have to convince a lot of people that that's the right way to do it. So Roshni, what I understand is that right now we are, what kind of a prediction we are getting from the vendor sources. Now we have to build our own mechanism so we cannot spend 2 1000000 or billions to our vendor partner and we can save our money. That's the ESG goal. Yeah, we don't have to, but we want to. 1. Last question. So this is the last. So, when I had a conversation, we talked about, as you said, that we are not hiring for the data scientists, you should be more focused on the ML ops. So here the, as you said, we have to go and build our standardization template and to end from designing development and how to roll out that model into a production, we have a proper standardization template, which other team can also take that leverage and follow it. So that, that's my understanding is correct. We don't have 1 to build. Yeah, we have to build on that side. So this is the on premises as well on the GCP right? This will be mostly on GCP. I don't know on prem, we'll see maybe, but for now my focus is on GCP. Thank you. Thank you, Roshni, for giving me this opportunity and Roshni 1 thing. I would probably join on the January 1 or 2 because I didn't get a chance because I was waiting to talk, I am waiting for this discussion because I was confused who will be my leader, whom I have to report and what my job role and the perspective because these days too much inflation is becoming There's a lot of pressure coming from my management, you have to get the list, who vendor partner you have to get up from the team, so this is very very challenging. So I was a little confused how my job would be saved if I join a new organization and all these questions. so definitely thank you for giving me an end to end response. Thank you so much and look forward to working with you and looking forward to working with you. That's great. If you have any more questions, feel free to email me or you have my email anyway so you can put in meetings and stuff. Just send me a message that you want to have their conversation. Thank you, Roshni. Thank you. We're all very excited for you to join us. Thank you and have a great day and enjoy your rest of the day. Bye. See you. Thank you. Bye bye. Bye.




====================Bharat=============================================================================

We have been through that situation, right? When we were doing the instant recommendation itself. we have been through that situation where we do not have any labels for the recommendation. For instant classification, you got the labels. They did manual work, right? At that point of time, we tried to get labels for the unstructured data. So when you are saying there are no labels, what kind of data set is it? Does it have textual data or is it some kind of numeric kind of data? So let's think about this as a textual data. So Firstly, you've done an unsupervised technique and then you tried to get some important keywords and the tagging. That's what you did in the first phase. 1st phase, I would say it in this manner. Initially, we would go with the clustering of similar sentences on a context basis. So each and every cluster will have its own thing. Okay. And the cluster names can be brought based on the keywords that are extracted from each of those things and get the mode of the keyword. We'll remove the stop words. See, you are making the similar context data into 1 cluster. From that cluster, try to remove the stop words and try to get the keywords out of it. Named LPR recognition. Out of those, get the most prominent keyword or ngram. Use a 2gram or 3gram frame to frame a word, label the data and make sure those labels are not on other labels. That's 1 way. Okay. That's a crude way of doing it. It really depends on the problem statement whether you really need labels or not. Even if it is numeric, right? Consider our anomaly detection. We did not have any data that indicates that this is an anomaly or this is a failed job. No, we have succeeded, failed and the kill job. Right? No, no, no. Actually, you have the kill job. Maybe in the spark. Okay. But in MAPR, right? Right. Kill Doesn't necessarily mean they are anomalous jobs.
 They sometimes start a few jobs and they do not want the job to run and even then they kill it. So that kind of creates a noise within your kill data. Right? So anyhow, you are ignoring the kill data when you're trying to generate the thresholds, right? We never consider the kill jobs because we 
never know whether they are really anomalous or not. There's a lot of noise in there. We'll use it for our, what do you call, validation purposes. Okay. Before that, what we do is within the data, we try to generate some thresholds based on the data that we have. And then we'll generate those thresholds and run the data through it.
 Only then will we be able to distinguish whether it's an anomaly or not. That is the label. Now we are creating the label to determine whether it's an anomaly or not. And that data can be used for the future purposes on how you want to use it. Okay. Thank you. Yeah. I'll try to think about this more. I have given the same, like 
I have also checked with the same thing. But the person, so my, so the person said to me, okay, what is an annotation? Annotation is costly for you. Let's say if this is not a case, if you find some other case where you are not dealing with the text data, how you do labeling and what are the different strategies you have to opt to do. I Just curious, 
I thought, okay, let me check with you, but we have to find out. And how do you deal with the ambiguity? Ambiguity is something that's really concerning for us. That's for sure. We did face that kind of ambiguity even while we are making the QnA maker. That Part I'm still exploring, to be honest. Because ambiguity is a serious issue for us. 
Right now it's small data, so we are getting along with it. But once we get into this complex data, it has a lot of things within each of those paragraphs. So we are not sure where to pick the answer from. That reason being: I'm exploring the data ambiguity, but given some time I can get some data back to you, some information on that. Thank you Bharat. Do not take too much time. So how do you cook? Do you cook your own or do you manage your food? Yeah, sometimes I cook and sometimes I just get food from outside. It's a task.
